{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkramAzouzi/masters_pfe/blob/main/Neural_Collaborative_Filtering_Model_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_4cfRKJ149Z"
      },
      "source": [
        "CF approach focuses on finding users who have given similar ratings to the same books, thus creating a link between users, to whom will be recommend books that were reviewed in a positive way. In this way, we look for associations between users, not between books."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project description\n",
        "In this project we aimed to construct a neural collaborative filtering recommender system with TensorFlow library, where recommendations of books are built upon the existing ratings of other users, who have similar ratings with the user to whom we want to recommend. This approach focuses on finding users who have given similar ratings to the same books, thus creating a link between users, to whom will recommend books that were reviewed in a positive way. In this way, we look for associations between users, not between books. Therefore, collaborative filtering relies only on observed user behavior to make recommendations â€” no profile data or content data is necessary."
      ],
      "metadata": {
        "id": "yR6asXaMMky_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Dataset"
      ],
      "metadata": {
        "id": "m_X2oUbeNBjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23cEaXqN2AEP"
      },
      "outputs": [],
      "source": [
        "!mkdir data && wget http://www2.informatik.uni-freiburg.de/~cziegler/BX/BX-CSV-Dump.zip && unzip BX-CSV-Dump.zip -d data/ && clear"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to import some libraries, since we are using google colab most of the python libraries came preinstalled, so we just need to import them."
      ],
      "metadata": {
        "id": "QzHEoS-hNIgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ2L0Zq9149h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading dataset tables from our storage"
      ],
      "metadata": {
        "id": "zjtO4faPOHnI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BonuCIL2149l"
      },
      "outputs": [],
      "source": [
        "rating = pd.read_csv('data/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
        "user = pd.read_csv('data/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
        "book = pd.read_csv('data/BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rating"
      ],
      "metadata": {
        "id": "dCmZYyLCD3ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After that we need to do some important steps to make the work clear, starting by Merging user, rating and book data and also removing unused columns."
      ],
      "metadata": {
        "id": "QYPjS6IEOm9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "book_rating = pd.merge(rating, book, on='ISBN')\n",
        "cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
        "book_rating.drop(cols, axis=1, inplace=True)\n",
        "book_rating.head()"
      ],
      "metadata": {
        "id": "Z_sI7ZvVOiHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then Filtering books that have had at least 25 ratings, Filtering users that have given at least 20 ratings."
      ],
      "metadata": {
        "id": "pwxeozYkO7jh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVUdcC0H149p"
      },
      "outputs": [],
      "source": [
        "#count all ratings\n",
        "rating_count = (book_rating.\n",
        "     groupby(by = ['Book-Title'])['Book-Rating'].\n",
        "     count().\n",
        "     reset_index().\n",
        "     rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
        "     [['Book-Title', 'RatingCount_book']]\n",
        "    )\n",
        "# rating_count.head()\n",
        "#books that have had at least 25 ratings\n",
        "threshold = 25\n",
        "rating_count = rating_count.query('RatingCount_book >= @threshold')\n",
        "user_rating = pd.merge(rating_count, book_rating, left_on='Book-Title', right_on='Book-Title', how='left')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5K1fsmHR149v"
      },
      "outputs": [],
      "source": [
        "#counting users\n",
        "user_count = (user_rating.\n",
        "     groupby(by = ['User-ID'])['Book-Rating'].\n",
        "     count().\n",
        "     reset_index().\n",
        "     rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
        "     [['User-ID', 'RatingCount_user']]\n",
        "    )\n",
        "# user_count.head()\n",
        "threshold = 20\n",
        "user_count = user_count.query('RatingCount_user >= @threshold')\n",
        "combined = user_rating.merge(user_count, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEv8Fh0c149z"
      },
      "outputs": [],
      "source": [
        "combined.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUik6bws1490"
      },
      "outputs": [],
      "source": [
        "print('Number of unique books: ', combined['Book-Title'].nunique())\n",
        "print('Number of unique users: ', combined['User-ID'].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMBLkIwH1490"
      },
      "source": [
        "Our technique will be based on the following observations:\n",
        "\n",
        "\n",
        "*   Users who rate books in a similar manner share one or more hidden preferences.\n",
        "*   Users with shared preferences are likely to give ratings in the same way to the same books.\n",
        "\n",
        "Now we pass to the Process in TensorFlow in order to normalize the ratings feature, then build user-book matrix with three features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdyVH9Rc1491"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "combined['Book-Rating'] = combined['Book-Rating'].values.astype(float)\n",
        "rating_scaled = pd.DataFrame(scaler.fit_transform(combined['Book-Rating'].values.reshape(-1,1)))\n",
        "combined['Book-Rating'] = rating_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59P6sJJ31491"
      },
      "source": [
        "Build the user book matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo0a_7yk1492"
      },
      "outputs": [],
      "source": [
        "combined = combined.drop_duplicates(['User-ID', 'Book-Title'])\n",
        "user_book_matrix = combined.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
        "user_book_matrix.fillna(0, inplace=True)\n",
        "\n",
        "users = user_book_matrix.index.tolist()\n",
        "books = user_book_matrix.columns.tolist()\n",
        "\n",
        "user_book_matrix = user_book_matrix.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9qxdbyI1492"
      },
      "source": [
        "tf.placeholder only available in v1, so we have to work around. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWWOpTJu1493",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64dda2d3-331b-4e8d-9577-3938f9b47f11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "os.system('clear')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following code script we aim to :\n",
        "* Set the network parameters, such as the dimension of each hidden layer.\n",
        "* Initialize the TF placeholder.\n",
        "* Weights and biases are randomly initialized.\n"
      ],
      "metadata": {
        "id": "3eQBhYIqQ6--"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa0YSJnr1493"
      },
      "source": [
        "We will initialize the TensorFlow placeholder. Then, weights and biases are randomly initialized, the following code are taken from the book: Python Machine Learning Cook Book - Second Edition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g6Fkklr1494"
      },
      "outputs": [],
      "source": [
        "num_input = combined['Book-Title'].nunique()\n",
        "num_hidden_1 = 10\n",
        "num_hidden_2 = 5\n",
        "\n",
        "X = tf.placeholder(tf.float64, [None, num_input])\n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
        "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
        "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
        "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
        "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
        "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
        "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCEuHB3I1495"
      },
      "source": [
        "Now, we can build the encoder and decoder model, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jlMM_Ef1495"
      },
      "outputs": [],
      "source": [
        "def encoder(x):\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
        "    return layer_2\n",
        "\n",
        "def decoder(x):\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
        "    return layer_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmEV0ohr1496"
      },
      "source": [
        "We will construct the model and the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9eAqQyo1496"
      },
      "outputs": [],
      "source": [
        "encoder_op = encoder(X)\n",
        "decoder_op = decoder(encoder_op)\n",
        "\n",
        "y_pred = decoder_op\n",
        "\n",
        "y_true = X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zM7hMNo1496"
      },
      "source": [
        "define loss function and optimizer, and minimize the squared error, and define the evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgBZQ8-01497"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
        "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
        "eval_x = tf.placeholder(tf.int32, )\n",
        "eval_y = tf.placeholder(tf.int32, )\n",
        "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIhbTaI31497"
      },
      "source": [
        "Initialize the variables. Because TensorFlow uses computational graphs for its operations, placeholders and variables must be initialized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tNI35zb1498"
      },
      "outputs": [],
      "source": [
        "init = tf.global_variables_initializer()\n",
        "local_init = tf.local_variables_initializer()\n",
        "pred_data = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPPjsq8Y1498"
      },
      "source": [
        "We can finally start to train our model.\n",
        "\n",
        "We split training data into batches, and we feed the network with them.\n",
        "\n",
        "We train our model with vectors of user ratings, each vector represents a user and each column a book, and entries are ratings that the user gave to books. \n",
        "\n",
        "After a few trials, We discovered that training model for 5 epochs with a batch size of 10 would be consum enough memory. This means that the entire training set will feed our neural network 20 times, every time using 50 users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4XYw9Q61498",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc75b7c3-9ae0-4ad9-d74c-da0613163bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 Loss: 0.3661415465585478\n",
            "epoch: 2 Loss: 0.3051663774710435\n",
            "epoch: 3 Loss: 0.062118540257010815\n",
            "epoch: 4 Loss: 0.004407464247708629\n",
            "epoch: 5 Loss: 0.003829436005696982\n",
            "epoch: 6 Loss: 0.003625660814897536\n",
            "epoch: 7 Loss: 0.003230570761773449\n",
            "epoch: 8 Loss: 0.0030120116412885242\n",
            "epoch: 9 Loss: 0.002924666096517755\n",
            "epoch: 10 Loss: 0.0027865328145428346\n",
            "epoch: 11 Loss: 0.0027303414094353934\n",
            "epoch: 12 Loss: 0.00272240114377832\n",
            "epoch: 13 Loss: 0.002716303084066117\n",
            "epoch: 14 Loss: 0.0027114765099403295\n",
            "epoch: 15 Loss: 0.0027075726948269123\n",
            "epoch: 16 Loss: 0.0027043569850950288\n",
            "epoch: 17 Loss: 0.0027016660103901893\n",
            "epoch: 18 Loss: 0.0026993829038014614\n",
            "epoch: 19 Loss: 0.00269742224078912\n",
            "epoch: 20 Loss: 0.002695720412533034\n",
            "epoch: 21 Loss: 0.0026942291693598194\n",
            "epoch: 22 Loss: 0.00269291143107054\n",
            "epoch: 23 Loss: 0.0026917384584321754\n",
            "epoch: 24 Loss: 0.0026906882348767184\n",
            "epoch: 25 Loss: 0.002689743662703332\n",
            "epoch: 26 Loss: 0.0026888912901855432\n",
            "epoch: 27 Loss: 0.00268811971373897\n",
            "epoch: 28 Loss: 0.002687418420164549\n",
            "epoch: 29 Loss: 0.0026867778941088325\n",
            "epoch: 30 Loss: 0.0026861901540861845\n",
            "epoch: 31 Loss: 0.002685648628387723\n",
            "epoch: 32 Loss: 0.0026851480987925944\n",
            "epoch: 33 Loss: 0.0026846839611387842\n",
            "epoch: 34 Loss: 0.002684252537469697\n",
            "epoch: 35 Loss: 0.002683850456858156\n",
            "epoch: 36 Loss: 0.002683474822993298\n",
            "epoch: 37 Loss: 0.002683123132306059\n",
            "epoch: 38 Loss: 0.0026827931844189273\n",
            "epoch: 39 Loss: 0.0026824829388655477\n",
            "epoch: 40 Loss: 0.002682190710821977\n",
            "epoch: 41 Loss: 0.002681914997123346\n",
            "epoch: 42 Loss: 0.0026816544289301547\n",
            "epoch: 43 Loss: 0.0026814078676749716\n",
            "epoch: 44 Loss: 0.0026811743820352213\n",
            "epoch: 45 Loss: 0.002680953171175833\n",
            "epoch: 46 Loss: 0.0026807434572889417\n",
            "epoch: 47 Loss: 0.002680544613522815\n",
            "epoch: 48 Loss: 0.0026803560104671415\n",
            "epoch: 49 Loss: 0.0026801769828915106\n",
            "epoch: 50 Loss: 0.002680006871961958\n",
            "epoch: 51 Loss: 0.0026798450303581224\n",
            "epoch: 52 Loss: 0.0026796907787774125\n",
            "epoch: 53 Loss: 0.002679543562008293\n",
            "epoch: 54 Loss: 0.002679402827397808\n",
            "epoch: 55 Loss: 0.002679268202672784\n",
            "epoch: 56 Loss: 0.002679139199144729\n",
            "epoch: 57 Loss: 0.0026790155149013788\n",
            "epoch: 58 Loss: 0.002678896830120421\n",
            "epoch: 59 Loss: 0.0026787828671960877\n",
            "epoch: 60 Loss: 0.0026786733152610914\n",
            "epoch: 61 Loss: 0.0026785679504398134\n",
            "epoch: 62 Loss: 0.0026784665565323697\n",
            "epoch: 63 Loss: 0.002678368894311671\n",
            "epoch: 64 Loss: 0.0026782747706050403\n",
            "epoch: 65 Loss: 0.0026781840178255853\n",
            "epoch: 66 Loss: 0.0026780965054858026\n",
            "epoch: 67 Loss: 0.002678011970052107\n",
            "epoch: 68 Loss: 0.0026779303795422663\n",
            "epoch: 69 Loss: 0.0026778514844948776\n",
            "epoch: 70 Loss: 0.002677775192801114\n",
            "epoch: 71 Loss: 0.002677701417469307\n",
            "epoch: 72 Loss: 0.0026776299896332754\n",
            "epoch: 73 Loss: 0.0026775609067344404\n",
            "epoch: 74 Loss: 0.0026774939423386047\n",
            "epoch: 75 Loss: 0.0026774290887700333\n",
            "epoch: 76 Loss: 0.0026773661873968585\n",
            "epoch: 77 Loss: 0.0026773051627410147\n",
            "epoch: 78 Loss: 0.002677246046784733\n",
            "epoch: 79 Loss: 0.002677188629724577\n",
            "epoch: 80 Loss: 0.002677132925632727\n",
            "epoch: 81 Loss: 0.0026770787886702097\n",
            "epoch: 82 Loss: 0.0026770262418642312\n",
            "epoch: 83 Loss: 0.0026769751892680976\n",
            "epoch: 84 Loss: 0.0026769255247008016\n",
            "epoch: 85 Loss: 0.002676877262234524\n",
            "epoch: 86 Loss: 0.002676830307201861\n",
            "epoch: 87 Loss: 0.0026767847005400684\n",
            "epoch: 88 Loss: 0.0026767402708243867\n",
            "epoch: 89 Loss: 0.002676697006541212\n",
            "epoch: 90 Loss: 0.002676654923042016\n",
            "epoch: 91 Loss: 0.0026766139870652785\n",
            "epoch: 92 Loss: 0.0026765740399791317\n",
            "epoch: 93 Loss: 0.002676535154703063\n",
            "epoch: 94 Loss: 0.0026764972288939324\n",
            "epoch: 95 Loss: 0.0026764603175611777\n",
            "epoch: 96 Loss: 0.002676424294055163\n",
            "epoch: 97 Loss: 0.0026763891916374094\n",
            "epoch: 98 Loss: 0.002676354887496148\n",
            "epoch: 99 Loss: 0.00267632148269523\n",
            "epoch: 100 Loss: 0.0026762888416299946\n"
          ]
        }
      ],
      "source": [
        "with tf.Session() as session:\n",
        "    epochs = 100\n",
        "    batch_size = 35\n",
        "\n",
        "    session.run(init)\n",
        "    session.run(local_init)\n",
        "\n",
        "    num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
        "    user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
        "    \n",
        "    for i in range(epochs):\n",
        "\n",
        "        avg_cost = 0\n",
        "        for batch in user_book_matrix:\n",
        "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
        "            avg_cost += l\n",
        "\n",
        "        avg_cost /= num_batches\n",
        "\n",
        "        print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
        "\n",
        "    user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
        "\n",
        "    preds = session.run(decoder_op, feed_dict={X: user_book_matrix})\n",
        "\n",
        "    pred_data = pred_data.append(pd.DataFrame(preds))\n",
        "\n",
        "    pred_data = pred_data.stack().reset_index(name='Book-Rating')\n",
        "    pred_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
        "    pred_data['User-ID'] = pred_data['User-ID'].map(lambda value: users[value])\n",
        "    pred_data['Book-Title'] = pred_data['Book-Title'].map(lambda value: books[value])\n",
        "    \n",
        "    keys = ['User-ID', 'Book-Title']\n",
        "    index_1 = pred_data.set_index(keys).index\n",
        "    index_2 = combined.set_index(keys).index\n",
        "\n",
        "    top_ten_ranked = pred_data[~index_1.isin(index_2)]\n",
        "    top_ten_ranked = top_ten_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
        "    top_ten_ranked = top_ten_ranked.groupby('User-ID').head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_ten_ranked['User-ID'].head(100)"
      ],
      "metadata": {
        "id": "sAvJNCVzS-xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok7b8Ki_1499"
      },
      "outputs": [],
      "source": [
        "top_ten_ranked.loc[top_ten_ranked['User-ID'] == 6543]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKXzFY8T3uPf"
      },
      "outputs": [],
      "source": [
        "# book_rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_-y1qav1499"
      },
      "outputs": [],
      "source": [
        "# book_rating.loc[book_rating['User-ID'] == 10314].sort_values(by=['Book-Rating'], ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZvvem5R149-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PFE Neural Collaborative Filtering Model with TensorFlow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}